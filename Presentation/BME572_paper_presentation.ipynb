{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communications</center></h1>\n",
    "<br>\n",
    "<br>\n",
    "<p>Andrew Van</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Motivation - Modeling Nonlinear systems\n",
    "\n",
    "- Linear systems follow 2 properties:\n",
    "    - Superposition: $f(x+y) = f(x) + f(y)$\n",
    "    - Homogeneity: $f(\\alpha x) = \\alpha f(x)$\n",
    "- Non-linear systems are those that are missing one or all of these properties.\n",
    "    - Because of this, it is generally difficult to study and obtain analytical models of non-linear systems\n",
    "    - However, we can still study/utilize non-linear systems through black boxes (given some input to my mystery box, what is the output?)\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/blackbox.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear systems (continued)\n",
    "\n",
    "- Most technical systems become non-linear at higher operational points (closer to saturation).\n",
    "    - More energy efficient, but because of non-linearities they become unpredicatable\n",
    "    - We use inefficient linear systems because of this\n",
    "    - Biomechanical systems use their full dynamic range (up to saturation) and are very efficient, and also throughly nonlinear.\n",
    "        - Inspirations from biology?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Echo State Networks (ESNs)\n",
    "\n",
    "- An approach to learning black-box models of nonlinear systems\n",
    "- A type of artificial recurrent neural network\n",
    "    - Feedback loops in their synaptic connections\n",
    "    - Can maintain activation in absence of input\n",
    "    - Exhibit dynamic memory\n",
    "    - Can learn to mimic any target system with arbitrary accuracy\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/recurrent.svg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What (was) novel about ESNs?\n",
    "\n",
    "- At the time of publication (2004), recurrent neural networks were very difficult to train\n",
    "    - Vanishing Gradient Problem\n",
    "    - Suboptimal Solutions due to contrained model complexity\n",
    "- ESNs eschew backprojection\n",
    "    - Create a large \"reservoir\" of neurons (50 - 1000) of random connections and only modify the input/output neurons\n",
    "    - No cyclic dependencies between trained readout connections, and training ESN becomes a simple linear regression task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demonstrating the utility of ESNs\n",
    "\n",
    "- Demonstrated ESN approach for a Mackey-Glass system (MGS)\n",
    "    - Standard benchmark system for time series prediction studies\n",
    "    - Generates an irregular time series\n",
    "- Two steps for using ESN\n",
    "    - Train with signal generated from original MGS as teacher signal\n",
    "    - Use it to predict original signal some steps ahead\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img src=\"images/mackeyglass.png\" width=\"33%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "- Create 1000 neurons (\"reservoir\") with sparse interconnections (1%) and one output neuron with random connections back into reservoir\n",
    "- 3000 step teacher sequence generated from original MGS, and fed into the output neuron\n",
    "    - Excites the internal neurons through the output feedback connections\n",
    "    - After initial transient period, exhibit systematic individual variations of the teacher sequence\n",
    "        - Act as \"echo functions\" for the driving signal\n",
    "        - Sparsity of interconnections lets reservoir decompose into many loosely coupled subsystems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training (continued)\n",
    "\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/F1.large.jpg\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training  (continued)\n",
    "\n",
    "- After time $n$ = 3000, output connection weights, $w_{i}$ were computed from the last 2000 steps ($n$ = 1001,...,3000) of the training run such that MSE was minimized\n",
    "\n",
    "$$ MSE_{train} = \\frac{1}{2000} \\sum_{n=1001}^{3000} \\Big(d(n) - \\sum_{i=1}^{1000} w_{i}x_{i}(n)\\Big)^{2} $$\n",
    "\n",
    "where $x_{i}(n)$ is the activation of the ith internal neuron at time $n$.\n",
    "\n",
    "- This is simple linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another Example (Signal Generator)\n",
    "\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/FreqGenSchema.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation\n",
    "\n",
    "- Disconnect after 3000 steps and left running freely\n",
    "- Output created through: $ y(n) = \\sum_{i=1}^{1000} w_{i}x_{i}(n) $\n",
    "- Looked at next 84-steps of original signal for comparison vs. generated signal.\n",
    "    - Averaged results over 100 independent trials, and calculated normalize root mean square error:\n",
    "\n",
    "$$ NRMSE = \\Big(\\sum_{j=1}^{100}(d_{j}(3084) - y_{i}(3084))^{2}/100 \\sigma^{2} \\Big)^{1/2} \\approx 10^{-4.2} $$\n",
    "\n",
    "- Improvement in performance by a factor of **700** when compared to previous techniques!\n",
    "- Deviations noted after about 1300 steps\n",
    "    - Refinements to model showed improvement factors up to **2400**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MGS Results\n",
    "<br>\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/F2.large.jpg\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why this jump in accuracy when compared to previous methods?\n",
    "\n",
    "- ESNs capitalize on a massive short-term memory.\n",
    "- Authors showed analytically, that under certain conditions, an ESN of size $N$ can \"remember\" a number of previous inputs that is of the same order of magnitude as $N$\n",
    "    - Further reading [here](https://opus.jacobs-university.de/frontdoor/index/index/docId/638)\n",
    "    - Significantly more memory than any other techniques (up until this point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A practical application of ESNs: Wireless Communication Channel Equalization\n",
    "\n",
    "- A sender wants to communicate a symbol sequence $s(n)$\n",
    "- This sequence is transformed in an analog envelope signal $d(n)$, then modulated on a high frequency carrier signal and transmitted\n",
    "- Receiver receives an analog signal $u(n)$, which is a corrupted version of $d(n)$\n",
    "    - Nonlinear distortion by operating the sender's power amplifier in the high gain region\n",
    "    - Avoided by running power amplification at below maximum amplification\n",
    "        - Imposed inefficiency due to nonlinear distortions!\n",
    "        - Nonlinear modeling can help run wireless communications more efficiently! (Cellphones/Satellites use less power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wireless Communication Channel Equalization\n",
    "\n",
    "- Equalizing filter to restore $u(n)$ as closely  as possible to $d(n)$, restored signal called $y(n)$\n",
    "    - Reconstruct symbol sequence from $y(n)$\n",
    "- Can we build a better equalizing filter that can handle nonlinear distortions?\n",
    "    - Authors used message sequences of 5000 symbols for training of ESN and compared against conventional methods of equalization\n",
    "    - Used 46 neurons to have comparable number of parameters as conventional models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results for Nonlinear Channel Equalization\n",
    "<br>\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/F3.large.jpg\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Echo State Networks\n",
    "\n",
    "- ESNs are highly configurable networks than can be used to model all kinds of nonlinear systems\n",
    "    - Signal and Processing Control, Time Series Prediction, Inverse Modeling, Pattern Generation, Event Detection and Classification, Modeling distributions of stochastic processes, filtering, and nonlinear control.\n",
    "- Very fast learning (~ few seconds to minutes depending on size of network)\n",
    "- Simple training process through linear regression\n",
    "- Biologically Inspired\n",
    "    - ESN principles not uncommon in biological networks\n",
    "    - Similar to models of prefrontal cortex function in sensory-motor sequencing tasks, models of birdsong, models of cerebellum, and general computational models of neural oscillators        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations and the Present\n",
    "\n",
    "- ESNs have been mostly supplanted by deep learning techniques, namely LSTM Networks\n",
    "    - ESNs were developed at a time when Vanishing Gradients were still an issue\n",
    "    - Compared to LSTMs, ESNs tend to perform worse on higher dimensional modeling tasks\n",
    "- Still useful?\n",
    "    - ESNs still popular when blended with non-digital computational substates\n",
    "        - Optical microchips, neuromorphic microchips, artificial soft limbs, etc.\n",
    "        - ESNs useful because interconnected ensemble of such devices (reservoir) can be built, and thus trained with ESN principles."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
