{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communications</center></h1>\n",
    "<br>\n",
    "<br>\n",
    "<p>Andrew Van</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Motivation - Modeling Nonlinear systems\n",
    "\n",
    "- Linear systems follow 2 properties:\n",
    "    - Superposition: $f(x+y) = f(x) + f(y)$\n",
    "    - Homogeneity: $f(\\alpha x) = \\alpha f(x)$\n",
    "- Non-linear systems are those that are missing one or all of these properties.\n",
    "    - Because of this, it is generally difficult to study and obtain analytical models of non-linear systems\n",
    "    - However, we can still study/utilize non-linear systems through black boxes (given some input to my mystery box, what is the output?)\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img src=\"images/blackbox.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear systems (continued)\n",
    "\n",
    "- Most technical systems become non-linear at higher operational points (closer to saturation).\n",
    "    - More energy efficient, but because of non-linearities they become unpredicatable\n",
    "    - We use inefficient linear systems because of this\n",
    "    - Biomechanical systems use their full dynamic range (up to saturation) and are very efficient, and also throughly nonlinear.\n",
    "        - Inspirations from biology?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Echo State Networks (ESNs)\n",
    "\n",
    "- An approach to learning black-box models of nonlinear systems\n",
    "- A type of artificial recurrent neural network\n",
    "    - Feedback loops in their synaptic connections\n",
    "    - Can maintain activation in absence of input\n",
    "    - Exhibit dynamic memory\n",
    "    - Can learn to mimic any target system with arbitrary accuracy\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img src=\"images/recurrent.svg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What (was) novel about ESNs?\n",
    "\n",
    "- At the time of publication (2004), recurrent neural networks were very difficult to train\n",
    "    - Vanishing Gradient Problem\n",
    "    - Suboptimal Solutions due to contrained model complexity\n",
    "- ESNs eschew backprojection\n",
    "    - Create a large \"reservoir\" of neurons (50 - 1000) of random connections and only modify the input/output neurons\n",
    "    - No cyclic dependencies between trained readout connections, and training ESN becomes a simple linear regression task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demonstrating the utility of ESNs\n",
    "\n",
    "- Demonstrated ESN approach for a Mackey-Glass system (MGS)\n",
    "    - Standard benchmark system for time series prediction studies\n",
    "    - Generates an irregular time series\n",
    "- Two steps for using ESN\n",
    "    - Train with signal generated from original MGS as teacher signal\n",
    "    - Use it to predict original signal some steps ahead\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img src=\"images/mackeyglass.png\" width=\"33%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "- Create 1000 neurons (\"reservoir\") with sparse interconnections (1%) and one output neuron with random connections back into reservoir\n",
    "- 3000 step teacher sequence generated from original MGS, and fed into the output neuron\n",
    "    - Excites the internal neurons through the output feedback connections\n",
    "    - After initial transient period, exhibit systematic individual variations of the teacher sequence\n",
    "        - Act as \"echo functions\" for the driving signal\n",
    "        - Sparsity of interconnections lets reservoir decompose into many loosely coupled subsystems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another Example (Signal Generator)\n",
    "\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img src=\"images/FreqGenSchema.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training (continued)\n",
    "\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img src=\"images/F1.large.jpg\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training  (continued)\n",
    "\n",
    "- After time $n$ = 3000, output connection weights, $w_{i}$ were computed from the last 2000 steps ($n$ = 1001,...,3000) of the training run such that MSE was minimized\n",
    "\n",
    "$$ MSE_{train} = \\frac{1}{2000} \\sum_{n=1001}^{3000} \\Big(d(n) - \\sum_{i=1}^{1000} w_{i}x_{i}(n)\\Big)^{2} $$\n",
    "\n",
    "where $x_{i}(n)$ is the activation of the ith internal neuron at time $n$.\n",
    "\n",
    "- This is simple linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation\n",
    "\n",
    "- Disconnect after 3000 steps and left running freely\n",
    "- Output created through: $ y(n) = \\sum_{i=1}^{1000} w_{i}x_{i}(n) $\n",
    "- Looked at next 84-steps of original signal for comparison vs. generated signal.\n",
    "    - Averaged results over 100 independent trials, and calculated normalize root mean square error:\n",
    "\n",
    "$$ NRMSE = \\Big(\\sum_{j=1}^{100}(d_{j}(3084) - y_{i}(3084))^{2}/100 \\sigma^{2} \\Big)^{1/2} \\approx 10^{-4.2} $$\n",
    "\n",
    "- Improvement in performance by a factor of **700** when compared to previous techniques!\n",
    "- Deviations noted after about 1300 steps\n",
    "    - Refinements to model showed improvement factors up to **2400**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MGS Results\n",
    "<br>\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img src=\"images/F2.large.jpg\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why this jump in accuracy when compared to previous methods?\n",
    "\n",
    "- ESNs capitalize on a massive short-term memory.\n",
    "- Authors showed analytically, that under certain conditions, an ESN of size $N$ can \"remember\" a number of previous inouts that is of the same order of magnitude as $N$\n",
    "    - Further reading [here](https://opus.jacobs-university.de/frontdoor/index/index/docId/638)\n",
    "    - Significantly more memory than any other techniques (up until this point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A practical application of ESNs\n",
    "\n",
    "- Wireless communications run at lower power profiles, since nonlinear distortion in high-gain power regions.\n",
    "    - Imposed inefficiency due to nonlinear distortions!\n",
    "    - Nonlinear modeling can help run wireless communications more efficiently! (Cellphones/Satellites use less power)\n",
    "- Used message sequences of 5000 symbols for training of ESN and compared against conventional methods of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results for Nonlinear Channel Equalization\n",
    "<br>\n",
    "<div style=\"width:fit-content;display:block;margin-right:auto;margin-left:auto;\">\n",
    "<img src=\"images/F3.large.jpg\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
